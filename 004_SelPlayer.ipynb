{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6b4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1303e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from players import RandomPlayer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19707d1",
   "metadata": {},
   "source": [
    "# SelfPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c40cc",
   "metadata": {},
   "source": [
    "En esta notebook se pide armar un entorno al cual se le pase como parámetro la clase de jugador local (DictPlayer, RandomPlayer, GreedyPlayer), y que el entorno devuelva el siguiente paso luego de jugar con el jugador local. Algunas condiciones:\n",
    "- En la función de reset(), se sortearea si el jugador local juega primero o segundo. \n",
    "- El entorno siempre devolverá el tablero como si le tocará jugar al jugador 1. Sea primero o segundo\n",
    "- La clase se instancia con los siguientes parámetros:\n",
    "    - LocalPlayer\n",
    "    - board_shape\n",
    "    \n",
    "- El método step recibirá como parámtro la acción pero codificada no como action = [columna, fila], si no como: action = action[0] * board_shape + action[1]\n",
    "- self.action_space tiene que estar definido acorde al espacio de acción. Por ejemplo: self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "- self.observation_space también: self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca7eb7",
   "metadata": {},
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea32127",
   "metadata": {},
   "source": [
    "El jugador local juega segundo entonces el reset() devuelve (Notar que no se devuelve el player por que siempre juega el 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f845064",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c4064",
   "metadata": {},
   "source": [
    "El jugador local juega primero entonces el reset() devuelve:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0607303",
   "metadata": {},
   "source": [
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb0c6024",
   "metadata": {},
   "source": [
    "Que ocurrio aca?\n",
    "\n",
    "El tablero se resetea y queda:\n",
    "\n",
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Luego el jugador local muestrea una de las cuatros opciones válidas y juega (0, 2) comiendo la pieza (1, 2) y tranformandola en 1\n",
    "\n",
    "[[ 0,  0,  1,  0],\n",
    " [ 0,  1,  1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora le toca el turno al jugador -1 pero el jugador externo tiene que ver el tablero como si fuera el 1, entonces se multiplica el tablero por -1\n",
    "\n",
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora el jugador externo seleccionará una acción observando el tablero como si fuera 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815d2c2",
   "metadata": {},
   "source": [
    "En cuanto a la recompenza tener en cuenta que deberá devolver:\n",
    "- 1 si gana el jugador externo\n",
    "- -1 si gana el LocalPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f1caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayEnv(ReversiEnv):\n",
    "    def __init__(self, board_shape=8, LocalPlayer=None):\n",
    "        super().__init__(board_shape=board_shape)\n",
    "        \n",
    "        self.players = [-1, 1]\n",
    "        self.local_player = LocalPlayer(board_shape=board_shape, flatten_action=False)\n",
    "        self.board_shape = board_shape\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape, board_shape))\n",
    "         \n",
    "        \n",
    "    def play(self, observation):\n",
    "        action = self.local_player.predict(observation)\n",
    "        (observation, self.current_player_num), reward, done, info = super(SelfPlayEnv, self).step(action)\n",
    "\n",
    "        return (observation, self.current_player_num), reward, done, info\n",
    "    \n",
    "    def encode_observation(self, observation, valid_actions=False):\n",
    "        # Implementar\n",
    "        # Simpre devuelve desde el punto de vista del jugador 1\n",
    "        # No devuleve el jugador sino solo el tablero\n",
    "        # Tener en cuenta que esto será la entrada a la red neuronal\n",
    "        board = observation * self.current_player_num\n",
    "        if valid_actions:\n",
    "            return np.array([board, self.get_valid((board, 1))])\n",
    "        else:\n",
    "            return board\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.local_player_num = np.random.choice(self.players)\n",
    "        self.local_player.player = self.local_player_num\n",
    "        self.observation, self.current_player_num = super().reset()\n",
    "        self.allow_pass = True\n",
    "            \n",
    "        if self.current_player_num == self.local_player_num:   \n",
    "            (self.observation, self.current_player_num), _, done, info = self.play(self.observation)\n",
    "            assert done == False\n",
    "\n",
    "        return self.encode_observation(self.observation)\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        # Esta es necesario ya que la salida de la red neuronal será un valor entre 0 y board_shape**2 - 1\n",
    "        return [action // self.board_shape, action % self.board_shape]\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        return action[0] * self.board_shape + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_step += 1\n",
    "        action = self.encode_action(action)\n",
    "        \n",
    "        (self.observation, self.current_player_num), reward, done, _ = super().step(action)   \n",
    "        \n",
    "            \n",
    "        while not done and (self.current_player_num == self.local_player_num):            \n",
    "            (self.observation, self.current_player_num), reward, done, info = self.play(self.observation)\n",
    "\n",
    "        \n",
    "        encoded_observation = self.encode_observation(self.observation)\n",
    "        reward = float(reward * -self.local_player_num)\n",
    "\n",
    "        return encoded_observation, reward, done, {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfPlayEnv(board_shape=4, LocalPlayer=RandomPlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b743dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0ce67",
   "metadata": {},
   "source": [
    "# Probar entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f86975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_valid_actions(state):\n",
    "    # np.argwhere junto con env.get_valid y randint solucionan el problema en pocas lineas pero puede usar otra estrategia\n",
    "    board_shape = state.shape[0]\n",
    "    # El player es siempre 1\n",
    "    player = 1\n",
    "    valid_actions = np.argwhere(env.get_valid((state, player)) == 1)\n",
    "    action = valid_actions[np.random.randint(len(valid_actions))]\n",
    "    return action[0] * board_shape + action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9184a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "action: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "action: 15\n",
      "[[ 0  0  0 -1]\n",
      " [ 0  1 -1  1]\n",
      " [ 0 -1  1  1]\n",
      " [ 0  0  0  1]]\n",
      "action: 2\n",
      "[[ 0 -1 -1 -1]\n",
      " [ 0 -1  1  1]\n",
      " [ 0 -1  1  1]\n",
      " [ 0  0  0  1]]\n",
      "action: 12\n",
      "[[ 0 -1 -1 -1]\n",
      " [ 0 -1  1  1]\n",
      " [ 0 -1  1  1]\n",
      " [ 1 -1  0  1]]\n",
      "action: 8\n",
      "[[ 0 -1 -1 -1]\n",
      " [ 0 -1 -1  1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1 -1 -1  1]]\n",
      "action: 4\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "board = env.reset()\n",
    "while not done:\n",
    "    action = sample_valid_actions(board)\n",
    "    print(board)\n",
    "    print(f'action: {action}')\n",
    "    \n",
    "    board, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57b6b5",
   "metadata": {},
   "source": [
    "# Entornos vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1beeb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65ee134",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66770fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31572e99",
   "metadata": {},
   "source": [
    "- Notar que la entrada tiene como primer componente la cantidad de entornos en paralelo (10), luego la cantidad de canales (1), y finalmente las dimensiones del tablero \n",
    "\n",
    "- Imprimir obs y ver que hay distintas posibles entradas dependiendo de quien juega primero y que jugó el LocalPlayer si le toco primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207639cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d19be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1. -1.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1. -1.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1. -1.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1. -1.  0.  0.  0.]\n",
      "   [ 0.  0.  0. -1.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  0.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17b346",
   "metadata": {},
   "source": [
    "### Guardar el SelfPlayEnv en el módulo multi_env para poder despues importarla desde otra notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c96a",
   "metadata": {},
   "source": [
    "# Instanciamos el modelo con MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eba29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40c2bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13685869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5a49022",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a646dd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ed0629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([47, 28, 31, 58, 41, 54,  0, 11, 54, 59]), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a64971",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Lo primero que hace stablebaselines si ponemos MLP es un flatten\n",
    "- Las acciones predichas por el modelo (sin entrentar) tienen una alta probabildad de ser inválidas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
