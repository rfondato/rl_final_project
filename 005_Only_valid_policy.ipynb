{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e30388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7375dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env, SelfPlayEnv\n",
    "import torch as th\n",
    "from players import RandomPlayer\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67958797",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'local_player_cls': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bf5fb",
   "metadata": {},
   "source": [
    "# Modificación de librería para que haga argmax solo sobre las válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f133b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    ActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d68253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([40, 13, 18, 26, 20, 26,  9, 58, 38, 47]), None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fbf4dd",
   "metadata": {},
   "source": [
    "# Custom ActorCriticPolicy \n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9985adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796e52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_not_vect = ReversiEnv(board_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b858e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(board, player) = env_not_vect.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8806c397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_not_vect.get_valid((board, player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bdd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_mask(state):\n",
    "    player = 1\n",
    "    valid_actions = env_not_vect.get_valid((state, player))\n",
    "    return valid_actions.reshape(-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de6e46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actions_mask(env.reset()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb40cf",
   "metadata": {},
   "source": [
    "## Importante: Aclaración para docentes\n",
    "Reemplacé esta sección de la notebook por una implementación en archivos .py para trabajarlo con un IDE.\n",
    "El enfoque que seguí fue el de utilizar un 2do canal para contener las máscaras, en vez de llamar a get_actions_mask en forward, predict, etc.\n",
    "Además utilizo SubprocVecEnv (en reemplazo de DummyVecEnv), que paraleliza el rollout de episodios en multiples procesos.\n",
    "Esto me permitió acelerar bastante el entrenamiento ya que las máscaras son calculadas en cada proceso y todo es convertido a tensores antes de entrar a la policy. Luego todo se ejecuta en GPU.\n",
    "\n",
    "Mirar los siguientes archivos:\n",
    "* custom_features_extractor.py => Contiene un features extractor (CustomBoardExtractor) que toma el board del canal 0 y hace un flatten, ignorando el canal 1 (mask), para alimentar las nn de actor-critic. También contiene uno para extraer features utilizando CNN (CNNFeaturesExtractor).\n",
    "* custom_policies.py => Contiene la implementación de CustomActorCritic modificada, donde se utiliza el canal 1 para aplicar la máscara de acciones válidas.\n",
    "* multi_env.py => Hice algunas modificaciones minimas a SelfPlayEnv para parametrizar el uso o no del canal de máscara, y además agregué poder entrenar contra distintos localPlayer en cada entorno paralelo. Esto me permite aprender de varias estrategias al mismo tiempo entre iteración e iteración, para evitar encontrar mínimos locales.\n",
    "* reversi_model.py => Contiene una clase de alto nivel (CustomReversiModel) para configurar, entrenar y guardar el modelo. Se utiliza en esta notebook.\n",
    "* learn.py => Script de entrada para entrenar por línea de comandos. Contiene múltiples argumentos que permiten customizar y rapidamente realizar un entrenamiento, creando un modelo nuevo o cargando uno preexistente, compitiendo contra una o múltiples estrategias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9bfb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversi_model import CustomReversiModel\n",
    "from multi_env import SelfPlayEnv\n",
    "from players import RandomPlayer\n",
    "import torch as th\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932051d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 8\n",
    "gamma = 0.99\n",
    "ent_coef = 0.001\n",
    "gae_lambda = 0.9\n",
    "n_epochs = 10\n",
    "n_steps = 2048\n",
    "learning_rate = 2e-4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb4deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = CustomReversiModel(board_shape=board_shape,\n",
    "                           n_envs=n_envs,\n",
    "                           local_player=RandomPlayer,\n",
    "                           n_steps=n_steps,\n",
    "                           n_epochs=n_epochs,\n",
    "                           learning_rate=learning_rate,\n",
    "                           ent_coef=ent_coef,\n",
    "                           gae_lambda=gae_lambda,\n",
    "                           batch_size=batch_size,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c98aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfPlayEnv(board_shape=board_shape, local_player_cls=RandomPlayer, mask_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3215a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23a995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array([obs]) #Convierto en un batch de 1 (1 env paralelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cc8186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0][0] #Tablero inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16eeec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([19], device='cuda:0'),\n",
       " tensor([[-0.4992]], device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " tensor([-1.0972], device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de forward\n",
    "model.model.policy(th.from_numpy(obs).to(model.model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f30de",
   "metadata": {},
   "source": [
    "# Corremos PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c1e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/PPO_65\n",
      "Eval num_timesteps=8000, episode_reward=-0.10 +/- 0.96\n",
      "Episode length: 30.01 +/- 0.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.1     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-0.09 +/- 0.96\n",
      "Episode length: 30.03 +/- 0.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.09    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 0.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 300      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=0.44 +/- 0.88\n",
      "Episode length: 30.17 +/- 0.63\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.2     |\n",
      "|    mean_reward          | 0.44     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 24000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00882  |\n",
      "|    clip_fraction        | 0.0479   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -2       |\n",
      "|    explained_variance   | -0.99    |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | 0.0151   |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | -0.0195  |\n",
      "|    value_loss           | 0.179    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=0.16 +/- 0.98\n",
      "Episode length: 30.02 +/- 0.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.16     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=0.40 +/- 0.91\n",
      "Episode length: 30.09 +/- 0.55\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | 0.4      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 40000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00778  |\n",
      "|    clip_fraction        | 0.039    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.99    |\n",
      "|    explained_variance   | 0.115    |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | 0.031    |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | -0.0195  |\n",
      "|    value_loss           | 0.135    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=0.42 +/- 0.87\n",
      "Episode length: 30.03 +/- 0.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.42     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.15     |\n",
      "| time/              |          |\n",
      "|    fps             | 259      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 29.83 +/- 2.47\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 29.8     |\n",
      "|    mean_reward          | 0.59     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 56000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00871  |\n",
      "|    clip_fraction        | 0.058    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.96    |\n",
      "|    explained_variance   | 0.322    |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | 0.0835   |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | -0.0224  |\n",
      "|    value_loss           | 0.129    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=0.71 +/- 0.65\n",
      "Episode length: 30.12 +/- 0.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.1     |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 64000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 254      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.55\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | 0.67     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 72000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00945  |\n",
      "|    clip_fraction        | 0.0713   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.95    |\n",
      "|    explained_variance   | 0.37     |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | 0.0252   |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | -0.0233  |\n",
      "|    value_loss           | 0.132    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=0.72 +/- 0.69\n",
      "Episode length: 30.13 +/- 0.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.1     |\n",
      "|    mean_reward     | 0.72     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 80000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 251      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=0.66 +/- 0.70\n",
      "Episode length: 29.98 +/- 0.42\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | 0.66     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 88000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00904  |\n",
      "|    clip_fraction        | 0.0724   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.92    |\n",
      "|    explained_variance   | 0.411    |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | -0.00436 |\n",
      "|    n_updates            | 50       |\n",
      "|    policy_gradient_loss | -0.0244  |\n",
      "|    value_loss           | 0.129    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.1     |\n",
      "|    mean_reward     | 0.68     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 250      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 393      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.14 +/- 0.51\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | 0.72     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 104000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00963  |\n",
      "|    clip_fraction        | 0.0779   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.91    |\n",
      "|    explained_variance   | 0.408    |\n",
      "|    learning_rate        | 0.0002   |\n",
      "|    loss                 | 0.0243   |\n",
      "|    n_updates            | 60       |\n",
      "|    policy_gradient_loss | -0.0239  |\n",
      "|    value_loss           | 0.126    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.10 +/- 0.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.1     |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 248      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 461      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ebcdc-d554-4247-bb82-b787ed0cf73b",
   "metadata": {},
   "source": [
    "#### Aclaración para docentes: Para mejores corridas referirse a la notebook 009_Analisis donde muestro todos los resultados de los diferentes experimentos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
