{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75725f40-2c8a-4b23-bbba-4dac40c93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3700508d-a5d0-44c1-8653-ffe2c9a8e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "from dynamic_programming import generate_uniform_stochastic_policy, policy_evaluation, stochastic_policy_eval_step, generate_deterministic_policy, deterministic_policy_eval_step\n",
    "from tree_search import bfs_cannonical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6626624-c808-4080-be16-a7b65e33755a",
   "metadata": {},
   "source": [
    "# Programación dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdf619-3c94-4d88-aa61-92e620995d4d",
   "metadata": {},
   "source": [
    "En esta parte no es necesario la implementación de código ya que ya esta todo resuelto. Si tiene que responder algunas preguntas en **EDX**.\n",
    "\n",
    "Si lo desea puede ver el código para analizar la implementación, pero es opcional\n",
    "\n",
    "Si quiere profundizar le recomendamos mirar:\n",
    "\n",
    "- bfs_cannonical cannonical de la librería tree_search\n",
    "- policy_evaluation, policy_improve, policy_iterartion y value_iteration de dynamic_programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae6c89-099e-4478-a9de-497e3ba485e8",
   "metadata": {},
   "source": [
    "### La idea de esta sección es generar las $V^*(s)$y $\\Pi^*(s)$ (óptimas) en 4x4 para poder hacer los análisis posteriores\n",
    "### Por eso se deben correr todas las celdas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663898a-15f8-445a-af04-3fa178c6c4ca",
   "metadata": {},
   "source": [
    "# Busqueda de todos los estados canónicos\n",
    "\n",
    "Solo desde el punto de vista del jugador +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a561791-2438-4746-afd2-ba0ed877f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 15.8 s, total: 2min 20s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "board_size = 4\n",
    "states = bfs_cannonical(board_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc6482-b962-48b8-bf4e-17eee0f9fd35",
   "metadata": {},
   "source": [
    "Al ser canónico, no es necesario que el jugador sea parte del estado ya que siempre se puede pensar como que le toca jugar al jugador +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c09c5-2a9f-4bb5-9bf7-45975a63e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Listamos los primeros 5 estados encontrados\n",
    "for s in list(states.keys())[0:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a9854f-8307-41a4-b1cc-f05bc0642466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el estado s0\n",
    "s0 = list(states.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae829c1-77da-48e1-a732-58334c1a2ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf7152c-20ab-4990-afaa-ed0ffdc768b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrado como tablero\n",
    "np.array(s0).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601082ff-abe4-46b0-8124-6664c18c51c3",
   "metadata": {},
   "source": [
    "Cada estado se guarda con todas sus posibles acciones y dado el estado y la acción, se guarda:\n",
    "- **next_node**: el próximo estado al ejecutar esa acción\n",
    "- **done**: si termina el juego (episodio)\n",
    "- **winner**: si al ejecutar la acción alguno de los dos jugadores gana: (+1 o -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447da39b-0540-408e-952c-ccf4de7a2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acción: (0, 2)\n",
      "{'done': False, 'winner': -0.0, 'next_node': (0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)}\n",
      "acción: (1, 3)\n",
      "{'done': False, 'winner': -0.0, 'next_node': (0, 0, 0, 0, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0)}\n",
      "acción: (2, 0)\n",
      "{'done': False, 'winner': -0.0, 'next_node': (0, 0, 0, 0, 0, -1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0)}\n",
      "acción: (3, 1)\n",
      "{'done': False, 'winner': -0.0, 'next_node': (0, 0, 0, 0, 0, -1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "for action, next_data in states[s0].items():\n",
    "    print(f'acción: {action}')\n",
    "    print(next_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301b4e1-e976-4f5e-8ac4-c23f9ddbb31d",
   "metadata": {},
   "source": [
    "# Ejemplo de un estado terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3e34dc-95ae-4946-8fb1-98e1a3334db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 -1 -1 -1]\n",
      " [ 0  1  1  0]\n",
      " [ 1  1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (0, 1, 1, 1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0)}\n",
      "\n",
      "[[ 0  0  0 -1]\n",
      " [ 0  1  1 -1]\n",
      " [ 0  1  1 -1]\n",
      " [ 0  1  0  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (0, 0, 0, 1, 0, -1, -1, 1, 0, -1, -1, 1, 0, -1, 0, 0)}\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [-1  1  1  0]\n",
      " [-1  1  1  0]\n",
      " [-1  0  0  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (0, 0, -1, 0, 1, -1, -1, 0, 1, -1, -1, 0, 1, 0, 0, 0)}\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0  1  1  0]\n",
      " [-1 -1 -1  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 1, 1, 1, 0)}\n",
      "\n",
      "[[-1 -1 -1  1]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (1, 1, 1, -1, 0, 1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0)}\n",
      "\n",
      "[[-1 -1 -1  0]\n",
      " [-1 -1  1  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0  0  0  0]]\n",
      "acción: (-1, 0)\n",
      "{'done': True, 'winner': 1, 'next_node': (1, 1, 1, 0, 1, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "done = 0\n",
    "for s in states.keys():\n",
    "    for action, next_data in states[s].items():\n",
    "        if next_data['done']:\n",
    "            print(np.array(s).reshape(4,4))\n",
    "            print(f'acción: {action}')\n",
    "            print(next_data)\n",
    "            done = done + 1\n",
    "            print()\n",
    "            break\n",
    "    if done > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62806c5b-8982-431b-acd9-1d6d2e46c934",
   "metadata": {},
   "source": [
    "La acción (-1, 0) es la acción PASS. En principio solo se ejecuta si no hay opciones válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3979263-db5d-4b37-8cd4-359a04644903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): {'done': True,\n",
       "  'winner': 1,\n",
       "  'next_node': (1, 1, 1, 0, 1, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[(-1, -1, -1, 0, -1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fbae2-641e-4c52-bcde-170cf96e86a5",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42e57b-692a-4df7-a9dc-54c3729d0d93",
   "metadata": {},
   "source": [
    "### Politica estocástica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33c4d35b-1e1d-4505-ae26-ea63f04b42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_pi = generate_uniform_stochastic_policy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc278a4b-e187-456c-88dc-61664c2fea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 2): 0.25, (1, 3): 0.25, (2, 0): 0.25, (3, 1): 0.25}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplos\n",
    "stochastic_pi[(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bb08f4-03c2-4714-abad-b64dc5f3ff80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.3333333333333333,\n",
       " (0, 3): 0.3333333333333333,\n",
       " (2, 3): 0.3333333333333333}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_pi[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5999b0-78fa-4745-a7d3-ca25192fa39c",
   "metadata": {},
   "source": [
    "Esto genera una política con distribución uniforme que luego será evaluada usuando **policy evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b26d10d-b7a3-47a2-a6f3-b88c7459f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n"
     ]
    }
   ],
   "source": [
    "V_stochastic, iters = policy_evaluation(stochastic_policy_eval_step, \n",
    "                             states, \n",
    "                             stochastic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3326e48-c8d4-475d-82ea-7c788765c870",
   "metadata": {},
   "source": [
    "#### Ejemplos de la V luego de converger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8530667a-01de-4d72-8131-623f48e31685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2403001935859148"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_stochastic[(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a520fe00-b246-4d76-a39e-fe0ead3bb601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2403001935859148"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_stochastic[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f372be8-8c59-4733-85b4-330315814478",
   "metadata": {},
   "source": [
    "### Política determinística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521a0c17-4af1-4bb6-8121-c61c52c280fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_pi = generate_deterministic_policy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5100a747-3333-4344-9de1-852e17c52a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_pi[(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c97a74-8398-43cb-9dd7-9b085ab66064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_pi[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb4f44-2315-4f74-b23d-d0df51894a8b",
   "metadata": {},
   "source": [
    "Notar que ahora la política dado el estado tiene solo una acción posible que se construyó de manera arbitraria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "101b1089-b9eb-421e-95a6-13d2a8fb62ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it takes different number of iterations to converge\n",
    "V_det, _ = policy_evaluation(deterministic_policy_eval_step, \n",
    "                             states, \n",
    "                             det_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b864354-cefc-424a-bcae-7d32bc50bc5f",
   "metadata": {},
   "source": [
    "#### Ejemplos de la V luego de converger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426b8f04-bedf-4d6f-b15f-d91449d0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_det[(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fdc2f72-512a-4e7f-bdea-eefee263a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_det[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca6f6b-944b-4c08-9452-be8b65dc673a",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Partiendo de cualquier política (estocástica o determinística), por medio de Policy Iteration se puede obtener las óptimas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf2950c-8947-44f0-89e7-f2ce9f81ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2db6733e-eb0d-4a33-a531-4684e74bae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 12656\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 \n",
      "Number of differences of new policy vs old policy: 2028\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 515\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 110\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 30\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 10\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 3\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n",
      "CPU times: user 23.5 s, sys: 1.23 s, total: 24.7 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initial_policy = generate_deterministic_policy(states)\n",
    "optimum_policy, optimum_V = policy_iteration(states, initial_policy, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36d741f6-1aba-4263-9c06-253fc77447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mdp/pi_mdp', optimum_policy)\n",
    "np.save('mdp/v_mdp', optimum_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1361e0c2-2b2e-4393-95ab-13152147fdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_policy[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d133ef7e-d058-4f93-a71c-b2aba2015791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_V[(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "005257ff-d1b9-43e0-a3d0-6b2659834ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1,  0],\n",
       "       [ 0, -1, -1,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0,  0,  0,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32444a3b-b383-41e8-b0ae-1978b70965dc",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5895e877-5331-42b3-b21c-651c3ccf9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e0c40c2-db8d-4368-86b7-af9f8413ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 2.148329015302604\n",
      "2 14 1.3984082309742596\n",
      "3 14 0.7103688654451921\n",
      "4 13 0.3661814318465639\n",
      "5 12 0.1380402974781458\n",
      "6 11 0.05770628692848223\n",
      "7 10 0.02005554416506682\n",
      "8 8 0.006710033363777003\n",
      "9 6 0.0023857896404540454\n",
      "10 6 0.0005964474101135114\n",
      "11 6 0.00011183388939628339\n",
      "12 0 0.0\n",
      "CPU times: user 7.8 s, sys: 0 ns, total: 7.8 s\n",
      "Wall time: 7.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "V, delta = value_iteration(states, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bff17-9cbe-485b-ba92-870cba8ffa21",
   "metadata": {},
   "source": [
    "# Cuestionario:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4e342-0472-447d-a0e8-9a68e06b9c12",
   "metadata": {},
   "source": [
    "### Pregunta 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabc8f1-1d67-41bb-8087-a63472710cf5",
   "metadata": {},
   "source": [
    "Dado el siguiente estado:\n",
    "\n",
    "(1, 1, 1, -1, 0, 1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0)\n",
    "\n",
    "y luego de ejecutar la acción:\n",
    "\n",
    "(3, 0)\n",
    "\n",
    "Evaluar **states** e indicar:\n",
    "\n",
    "¿Cuánto vale **done**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b3433d2-6e6c-430f-b9da-8979de1a0095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1 -1]\n",
      " [ 0  1  1  0]\n",
      " [ 0 -1  1  1]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "state = (1, 1, 1, -1, 0, 1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0)\n",
    "print(np.array(state).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cacb67f-f9a3-4bb6-af77-edb959336ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'done': False,\n",
       "  'winner': -0.0,\n",
       "  'next_node': (-1, -1, -1, 1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0)},\n",
       " (3, 0): {'done': True,\n",
       "  'winner': -1,\n",
       "  'next_node': (-1, -1, -1, 1, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0)},\n",
       " (3, 1): {'done': False,\n",
       "  'winner': -0.0,\n",
       "  'next_node': (-1, -1, -1, 1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0)}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe9c0c69-dc2e-4def-960a-fb93ae3e64fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'done': True,\n",
       " 'winner': -1,\n",
       " 'next_node': (-1, -1, -1, 1, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[state][(3, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6042745-f8ac-449a-b766-4f49391faf9f",
   "metadata": {},
   "source": [
    "### Pregunta 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8747e-9414-49c4-aaf5-d3efedd4c5a1",
   "metadata": {},
   "source": [
    "Luego de generara una política estocástica con distribución uniforme.\n",
    "\n",
    "Dado el estado (0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
    "\n",
    "¿Cuánto es la probabilidad de la acción (0, 3) ?\n",
    "\n",
    "¿Cuánto es la probabilidad de la acción (3, 3) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5d05272-7c8f-42c2-aeec-e36a1e1f0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -1  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "state = (0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
    "print(np.array(state).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17ef439c-453e-4a9c-ac30-009208f0470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_pi = generate_uniform_stochastic_policy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffb64390-9896-46df-b739-aa15e35010dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.3333333333333333,\n",
       " (0, 3): 0.3333333333333333,\n",
       " (2, 3): 0.3333333333333333}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_pi[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a68b46-dfe2-487d-8ec0-af5ad46f3be9",
   "metadata": {},
   "source": [
    "### Pregunta 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c4788-7bae-49a4-905f-d17e88641ffd",
   "metadata": {},
   "source": [
    "Luego de evaluar las dos políticas (correr policy_evaluation) (determinística y estocástica) se pueden sacar las siguientes conclusiones:\n",
    "\n",
    "La evaluación de cualquier estado de la política determinística siempre da 1, 0, o -1.\n",
    "\n",
    "La evaluación de cualquier estado de la política estocástica siempre da 1, 0, o -1.\n",
    "\n",
    "La evaluación de cualquier estado de la política determinística no tiene por que dar un número entero.\n",
    "\n",
    "La evaluación de cualquier estado de la política estocástica no tiene por que dar un número entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15245de0-d18d-47a9-a052-3d432f7c975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 \n"
     ]
    }
   ],
   "source": [
    "V_det, _ = policy_evaluation(deterministic_policy_eval_step, \n",
    "                             states, \n",
    "                             det_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "310401bc-a873-477d-94df-f9ee7731e128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n"
     ]
    }
   ],
   "source": [
    "V_stochastic, _ = policy_evaluation(stochastic_policy_eval_step, \n",
    "                             states, \n",
    "                             stochastic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5efd2427-e44d-4014-9ec3-b8b4de037e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(V_det.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91dc1361-873c-4484-98e8-44f335ae0c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.98611111, -0.98611111, -0.97916667, -0.97569444,\n",
       "       -0.97222222, -0.97222222, -0.96875   , -0.95833333, -0.94814815])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(V_stochastic.values()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c95413-c63c-41ef-affc-f2f184fd5d96",
   "metadata": {},
   "source": [
    "### Pregunta 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d61be-29a0-4b22-b4cb-949119b4e4fe",
   "metadata": {},
   "source": [
    "Luego de correr policy iteration. Evaluar el estado:\n",
    "\n",
    "(0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
    "\n",
    "¿Cuánto vale la value de ese estado?\n",
    "\n",
    "¿Cual es la jugada ganadora en ese estado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b24fb49-57e9-4ae7-a16f-060e7e5c3e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -1  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "state = (0, 0, -1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
    "print(np.array(state).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f528072-b85e-426d-ac7c-9a946449f506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_V[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fab0e1c-ebb3-484c-974c-5020c3e9f6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_policy[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551dce35-3b68-4c40-b55c-f187d715e97c",
   "metadata": {},
   "source": [
    "### Pregunta 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0110b1-4877-43eb-ae87-21a40021f01f",
   "metadata": {},
   "source": [
    "Dado un rápido análisis de la V y PI optimas, indicar cual de las siguientes afirmaciones son correctas:\n",
    "\n",
    "El que juega segundo gana solo si juega en una posición que come en diagonal.\n",
    "\n",
    "El que juega segundo gana independientemente de lo que juegue.\n",
    "\n",
    "Si el primero juega bien, puede lograr un empate.\n",
    "\n",
    "Si el segundo no juega bien en su primer turno, y el primero juega bien en su segundo turno, éste (el primero) ganará la partida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4075ab95-0f86-49a4-90c3-13c22d94fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state, values, policy, player, inv = 1):\n",
    "    current_player_wins = values[state] == 1\n",
    "    other_player = 3 - player\n",
    "    win_player = player if current_player_wins else other_player\n",
    "    print((inv * np.array(state)).reshape(4,4))\n",
    "    print(f\"Winner will be player: {win_player}\")\n",
    "    print(f\"Best move: {policy[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47697ea5-051a-40a3-8934-180d09cfaafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 2)\n"
     ]
    }
   ],
   "source": [
    "state = (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
    "print_state(state, optimum_V, optimum_policy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3386463a-0fe8-4a63-b13c-34df3f41d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_game_tree(states, values, op_policy, state = (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0), player = 1, round = 0, action=None, max_rounds = 3):\n",
    "    if (round > max_rounds):\n",
    "        return\n",
    "    \n",
    "    inv = 1 + (1-player)*2\n",
    "    next_player = 3 - player\n",
    "    \n",
    "    print(\"#\"*40)\n",
    "    print(\"Current Round: \", \"Initial\" if (round == 0) else round)\n",
    "    print(\"Current player: \", player)\n",
    "    if (action is not None):\n",
    "        print(\"Prev move: \", action)\n",
    "    print(\"Current board: \")\n",
    "    print_state(state, values, op_policy, player, inv)\n",
    "    print(\"#\"*40)\n",
    "    print()\n",
    "    \n",
    "    actions = states[state]\n",
    "    next_round = round + 1\n",
    "    \n",
    "    print(f\"Posible moves for player: {player}\")\n",
    "    for (a, data) in actions.items():\n",
    "        next_state = data['next_node']\n",
    "        print(\"If it moves to: \", a)\n",
    "        print_state(next_state, values, op_policy, next_player, inv * -1)\n",
    "        print()\n",
    "    \n",
    "    for (a, data) in actions.items():\n",
    "        next_state = data['next_node']\n",
    "        done = data['done']\n",
    "        if (not done):\n",
    "            print_game_tree(states, values, op_policy, state=next_state, player=next_player, round = next_round, action = a, max_rounds = max_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6ba690d-0d0c-45c5-99a0-4b7c0da46d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Current Round:  Initial\n",
      "Current player:  1\n",
      "Current board: \n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 2)\n",
      "########################################\n",
      "\n",
      "Posible moves for player: 1\n",
      "If it moves to:  (0, 2)\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 3)\n",
      "\n",
      "If it moves to:  (1, 3)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 3)\n",
      "\n",
      "If it moves to:  (2, 0)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 1  1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (3, 0)\n",
      "\n",
      "If it moves to:  (3, 1)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0  1  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (3, 0)\n",
      "\n",
      "########################################\n",
      "Current Round:  1\n",
      "Current player:  2\n",
      "Prev move:  (0, 2)\n",
      "Current board: \n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 3)\n",
      "########################################\n",
      "\n",
      "Posible moves for player: 2\n",
      "If it moves to:  (0, 1)\n",
      "[[ 0 -1  1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (0, 0)\n",
      "\n",
      "If it moves to:  (0, 3)\n",
      "[[ 0  0  1 -1]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (1, 3)\n",
      "\n",
      "If it moves to:  (2, 3)\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (3, 0)\n",
      "\n",
      "########################################\n",
      "Current Round:  1\n",
      "Current player:  2\n",
      "Prev move:  (1, 3)\n",
      "Current board: \n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 3)\n",
      "########################################\n",
      "\n",
      "Posible moves for player: 2\n",
      "If it moves to:  (0, 1)\n",
      "[[ 0 -1  0  0]\n",
      " [ 0 -1  1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (0, 0)\n",
      "\n",
      "If it moves to:  (0, 3)\n",
      "[[ 0  0  0 -1]\n",
      " [ 0  1 -1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 2)\n",
      "\n",
      "If it moves to:  (2, 3)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  1  1]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (3, 3)\n",
      "\n",
      "########################################\n",
      "Current Round:  1\n",
      "Current player:  2\n",
      "Prev move:  (2, 0)\n",
      "Current board: \n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 1  1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (3, 0)\n",
      "########################################\n",
      "\n",
      "Posible moves for player: 2\n",
      "If it moves to:  (1, 0)\n",
      "[[ 0  0  0  0]\n",
      " [-1 -1 -1  0]\n",
      " [ 1  1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (0, 0)\n",
      "\n",
      "If it moves to:  (3, 0)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 1 -1  1  0]\n",
      " [-1  0  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 2)\n",
      "\n",
      "If it moves to:  (3, 2)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 1  1 -1  0]\n",
      " [ 0  0 -1  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (0, 3)\n",
      "\n",
      "########################################\n",
      "Current Round:  1\n",
      "Current player:  2\n",
      "Prev move:  (3, 1)\n",
      "Current board: \n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0  1  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (3, 0)\n",
      "########################################\n",
      "\n",
      "Posible moves for player: 2\n",
      "If it moves to:  (1, 0)\n",
      "[[ 0  0  0  0]\n",
      " [-1 -1 -1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0  1  0  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (0, 0)\n",
      "\n",
      "If it moves to:  (3, 0)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [-1  1  0  0]]\n",
      "Winner will be player: 2\n",
      "Best move: (0, 2)\n",
      "\n",
      "If it moves to:  (3, 2)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  1 -1  0]]\n",
      "Winner will be player: 1\n",
      "Best move: (3, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_game_tree(states, optimum_V, optimum_policy, max_rounds = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfc5a1-b48a-4292-aedc-4ed8dd393fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
