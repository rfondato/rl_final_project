{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d94b8e",
   "metadata": {},
   "source": [
    "# Crear un TorchPlayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b941d2",
   "metadata": {},
   "source": [
    "Recibe el modelo a instanciar como path y juega con el mismo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed459cfe",
   "metadata": {},
   "source": [
    "- Pensar como resolver el problema de que solo samplee las válidas\n",
    "- Agregarle la opción de monte carlo tree search (opcional) con las opciones de iterationLimit, timeLimit\n",
    "\n",
    "Si va a agregar MCTS mirar la notebook 007_MCTS.ipnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64c973-213b-459a-8c18-5993e8d3afe6",
   "metadata": {},
   "source": [
    "## Aclaraciones para docentes:\n",
    "TorchPlayer está tambien en players.py pero la copio acá para mayor claridad de la notebook. Hereda de BasePlayer, clase que está definida en players.py y contiene una interfaz común que utilizo en clases como Arena (arena.py) y Tournament (tournament.py), para jugar partidos. Además posee un par de métodos extra por el mismo motivo.\n",
    "\n",
    "Reescribí monte carlo tree search en un archivo nuevo (multi_process_mcts.py) e hice los siguientes cambios:\n",
    "* MultiProcessMonteCarlo:\n",
    "    * Nueva clase que puede correr muchas simulaciones en paralelo utilizando múltiples procesos. \n",
    "    * Además cambia el sistema de límites para ser por profundidad (levelLimit) en vez de iteraciones o tiempo. \n",
    "        * Si el nivel de profundidad (levelLimit) es 1, por cada step toma todos los siguientes posibles nodos en base a acciones validas (por lo general 3 o 4) y juega una partida contra si mismo en cada uno utilizando el modelo actual, eligiendo luego la mejor opción. \n",
    "        * Si es 2, toma todos los hijos de los hijos y juega todas esas posibilidades, y luego en base a un reward ponderado por visitas elige la mejor acción.\n",
    "    * El motivo es que correr tantas simulaciones para tomar cada decisión era muy lento, y por eso al utilizarlo con levelLimit=1 y multiples procesos se le da la misma importancia a cada rama del árbol y se corren 3 o 4 juegos en paralelo por movida, acelerando mucho el desarrollo del partido.\n",
    "* CustomReversiState => Reversión de ReversiState con algunos cambios, por ej no hacer deep cloning del env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5e1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566714bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Optional, Dict, Any\n",
    "from boardgame2 import BoardGameEnv\n",
    "from stable_baselines3 import PPO\n",
    "from multi_process_mcts import MultiProcessMonteCarlo, model_policy\n",
    "from reversi_state import CustomReversiState\n",
    "from players import BasePlayer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1874e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPlayer(BasePlayer):\n",
    "    def __init__(self,\n",
    "                 player: int = 1,\n",
    "                 env: BoardGameEnv = None,\n",
    "                 flatten_action: bool = False,\n",
    "                 model_path: str = None,\n",
    "                 deterministic: bool = True,\n",
    "                 only_valid: bool = True,\n",
    "                 mcts: bool = False,\n",
    "                 levelLimit: int = None,\n",
    "                 device: str = 'auto',\n",
    "                 mtcs_n_processes: int = None\n",
    "                 ):\n",
    "\n",
    "        if model_path is None:\n",
    "            raise Exception(\"model_path cannot be None\")\n",
    "\n",
    "        super().__init__(player, env, flatten_action, os.path.splitext(os.path.basename(model_path))[0])\n",
    "\n",
    "        self.model = PPO.load(model_path, device=device)\n",
    "        self.model_path = model_path\n",
    "        self.deterministic = deterministic\n",
    "        self.only_valid = only_valid\n",
    "        self.mcts = mcts\n",
    "        self.levelLimit = levelLimit\n",
    "        self.mtcs_n_processes = mtcs_n_processes\n",
    "\n",
    "    def predict(self, board: np.ndarray) -> Union[int, np.ndarray]:\n",
    "        if self.mcts:\n",
    "            action = self._get_action_with_mcts(board)\n",
    "            action = action.action\n",
    "            if self.flatten_action:\n",
    "                return action[0] * self.board_shape + action[1]\n",
    "            else:\n",
    "                return action\n",
    "        else:\n",
    "            obs = self.player * board\n",
    "            if self.only_valid:\n",
    "                obs = [obs, self.env.get_valid((obs, 1))]\n",
    "            # The model expects a batch of observations.\n",
    "            # Make a batch of 1 obs\n",
    "            obs = [obs]\n",
    "            action = self.model.predict(obs, deterministic=self.deterministic)[0]\n",
    "\n",
    "            if self.flatten_action:\n",
    "                return action\n",
    "            else:\n",
    "                return np.array([action // self.board_shape, action % self.board_shape])\n",
    "\n",
    "    def _get_action_with_mcts(self, board: np.ndarray) -> Union[int]:\n",
    "        searcher = MultiProcessMonteCarlo(levelLimit=self.levelLimit,\n",
    "                                          n_processes=self.mtcs_n_processes,\n",
    "                                          explorationConstant=0.0,\n",
    "                                          rolloutPolicy=model_policy(self.model))\n",
    "\n",
    "        state = CustomReversiState(self.env, (board, self.player))\n",
    "        return searcher.search(initialState=state)\n",
    "\n",
    "    def __str__(self):\n",
    "        monte_carlo = f\"- MCTS\" if self.mcts else \"\"\n",
    "        return f\"{self.__class__.__name__}({self.name}{monte_carlo})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52660ebc",
   "metadata": {},
   "source": [
    "# Arena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ccf23",
   "metadata": {},
   "source": [
    "Testear el jugador contra los distintos jugadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbee1c-d583-4f08-8042-d0621e9811c2",
   "metadata": {},
   "source": [
    "#### Aclaraciones para docentes: En arena.py se puede encontrar una implementación de una clase \"Arena\" que permite correr partidas entre 2 jugadores de cualquier clase, imprimir resultados y almacenar estadísticas. Utilizo el mejor modelo para ver los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4502e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arena import Arena\n",
    "from players import RandomPlayer, GreedyPlayer, TorchPlayer\n",
    "from boardgame2 import ReversiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66a1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ReversiEnv(board_shape=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23c108-ddd3-4fd5-b7a5-64059f192772",
   "metadata": {},
   "source": [
    "## Torch vs Random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a113d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = TorchPlayer(player=1, env=env, model_path=\"./Vs_Multiple_v2.zip\")\n",
    "player_2 = RandomPlayer(player=-1, env=env)\n",
    "arena = Arena(player_1, player_2, env, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35ccbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCH: TorchPlayer(Vs_Multiple_v2) vs RandomPlayer\n",
      "\n",
      "\u001b[KPlaying n:100/100 \t Wins(player 1/ player 2):92.93%/2.02% \t Ties:5.05%\n",
      "\n",
      "THE WINNER IS TorchPlayer(Vs_Multiple_v2)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arena.play(n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffe1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### STATS FOR PLAYER: 1 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.8775510204081632\n",
      "Wins as second: 0.9803921568627451\n",
      "Ties: 0.05\n",
      "Plays as first: 49\n",
      "Plays as second: 51\n",
      "Avg game duration: 59.61\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n",
      "\n",
      "####### STATS FOR PLAYER: 2 - RandomPlayer #######\n",
      "\n",
      "Wins as first: 0.0196078431372549\n",
      "Wins as second: 0.02040816326530612\n",
      "Ties: 0.05\n",
      "Plays as first: 51\n",
      "Plays as second: 49\n",
      "Avg game duration: 59.61\n",
      "\n",
      "##################################################\n",
      "            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "arena.print_players_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a5a39-6b99-4f5d-a41a-e5dff3bf3163",
   "metadata": {},
   "source": [
    "#### Gana cerca del 88% como primero y 98% como segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee52b0-0e01-4328-b276-ea110389f742",
   "metadata": {},
   "source": [
    "## Torch vs Greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb2d415-b7a4-4b43-b3ef-2370b84ff95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = TorchPlayer(player=1, env=env, model_path=\"./Vs_Multiple_v2.zip\")\n",
    "player_2 = GreedyPlayer(player=-1, env=env)\n",
    "arena = Arena(player_2, player_1, env, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee8540e4-451e-4f24-aadd-08eb5088cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCH: GreedyPlayer vs TorchPlayer(Vs_Multiple_v2)\n",
      "\n",
      "\u001b[KPlaying n:100/100 \t Wins(player 1/ player 2):6.06%/92.93% \t Ties:1.01%\n",
      "\n",
      "THE WINNER IS TorchPlayer(Vs_Multiple_v2)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arena.play(n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b87882-bc75-4749-992b-31d9946ed055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### STATS FOR PLAYER: 1 - GreedyPlayer #######\n",
      "\n",
      "Wins as first: 0.11538461538461539\n",
      "Wins as second: 0.0\n",
      "Ties: 0.01\n",
      "Plays as first: 52\n",
      "Plays as second: 48\n",
      "Avg game duration: 58.81\n",
      "\n",
      "##################################################\n",
      "            \n",
      "        \n",
      "\n",
      "####### STATS FOR PLAYER: 2 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.9791666666666666\n",
      "Wins as second: 0.8846153846153846\n",
      "Ties: 0.01\n",
      "Plays as first: 48\n",
      "Plays as second: 52\n",
      "Avg game duration: 58.81\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "arena.print_players_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd52b5-a4b0-4855-bde0-702b576aca91",
   "metadata": {},
   "source": [
    "#### Se comporta similar que vs Random. Juega mejor como segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ba34d-6d5b-493a-8041-997894e147eb",
   "metadata": {},
   "source": [
    "## Torch vs Torch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed2951-ace8-44eb-aa5f-fbc3e320b514",
   "metadata": {},
   "source": [
    "### No deterministico: Más interesante, sino juega siempre el mismo partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22bcea2d-2517-418d-b604-ab2244c20caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = TorchPlayer(player=1, env=env, model_path=\"./Vs_Multiple_v2.zip\", deterministic=False)\n",
    "player_2 = TorchPlayer(player=-1, env=env, model_path=\"./Vs_Multiple_v2.zip\", deterministic=False)\n",
    "arena = Arena(player_1, player_2, env, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9dae7b-02bf-4a37-950e-564ebfa89c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCH: TorchPlayer(Vs_Multiple_v2) vs TorchPlayer(Vs_Multiple_v2)\n",
      "\n",
      "\u001b[KPlaying n:100/100 \t Wins(player 1/ player 2):54.55%/42.42% \t Ties:3.03%\n",
      "\n",
      "THE WINNER IS TorchPlayer(Vs_Multiple_v2)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arena.play(n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2e2468-adb2-488c-b754-f970c749d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### STATS FOR PLAYER: 1 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.48148148148148145\n",
      "Wins as second: 0.6304347826086957\n",
      "Ties: 0.03\n",
      "Plays as first: 54\n",
      "Plays as second: 46\n",
      "Avg game duration: 59.9\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n",
      "\n",
      "####### STATS FOR PLAYER: 2 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.32608695652173914\n",
      "Wins as second: 0.5\n",
      "Ties: 0.03\n",
      "Plays as first: 46\n",
      "Plays as second: 54\n",
      "Avg game duration: 59.9\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "arena.print_players_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7705130-6eb8-4abb-8f60-fc399e3f2e37",
   "metadata": {},
   "source": [
    "#### Esta parejo, jugando tanto de primero como segundo, que es lo esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e36e6-78ea-4785-8117-8d7ca33209e4",
   "metadata": {},
   "source": [
    "## Deterministico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e5333f-bb99-47bb-ad79-64e11e03c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = TorchPlayer(player=1, env=env, model_path=\"./Vs_Multiple_v2.zip\")\n",
    "player_2 = TorchPlayer(player=-1, env=env, model_path=\"./Vs_Multiple_v2.zip\")\n",
    "arena = Arena(player_1, player_2, env, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccc634bc-7d74-4cbc-8816-e71434367bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCH: TorchPlayer(Vs_Multiple_v2) vs TorchPlayer(Vs_Multiple_v2)\n",
      "\n",
      "\u001b[KPlaying n:100/100 \t Wins(player 1/ player 2):54.55%/45.45% \t Ties:0.0%\n",
      "\n",
      "THE WINNER IS TorchPlayer(Vs_Multiple_v2)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arena.play(n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af78ebc6-1066-4aaa-a71e-0dce983b0f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### STATS FOR PLAYER: 1 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.0\n",
      "Wins as second: 1.0\n",
      "Ties: 0.0\n",
      "Plays as first: 46\n",
      "Plays as second: 54\n",
      "Avg game duration: 60.0\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n",
      "\n",
      "####### STATS FOR PLAYER: 2 - TorchPlayer(Vs_Multiple_v2) #######\n",
      "\n",
      "Wins as first: 0.0\n",
      "Wins as second: 1.0\n",
      "Ties: 0.0\n",
      "Plays as first: 54\n",
      "Plays as second: 46\n",
      "Avg game duration: 60.0\n",
      "\n",
      "#################################################################\n",
      "            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "arena.print_players_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c70ec9-e683-4d61-865a-810a882f7c6b",
   "metadata": {},
   "source": [
    "#### Se gana a si mismo el 100% si arranca segundo. Esto es simplemente porque al ser los 2 deterministicos juegan el mismo juego una y otra vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794d38c-a6c6-470c-b5c2-0c3b791a1706",
   "metadata": {},
   "source": [
    "## Torch vs Torch con Monte Carlo: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ccf2d-b750-4f85-b695-f0fcec999ee8",
   "metadata": {},
   "source": [
    "#### Jugar con Monte Carlo toma mucho tiempo. Pruebo con 1 solo nivel de profundidad y a 50 juegos. Modo No deterministico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02a283a-a4b2-4e0c-b41d-7f4322fdb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = TorchPlayer(player=1, env=env, model_path=\"./Vs_Multiple_v2.zip\", deterministic=False)\n",
    "player_2 = TorchPlayer(player=-1, env=env, model_path=\"./Vs_Multiple_v2.zip\", deterministic=False, mcts=True, levelLimit=1, device=\"cpu\")\n",
    "arena = Arena(player_1, player_2, env, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35084b06-7cc8-4f74-9e9e-a7ca9f0eb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATCH: TorchPlayer(Vs_Multiple_v2) vs TorchPlayer(Vs_Multiple_v2- MCTS)\n",
      "\n",
      "\u001b[KPlaying n:50/50 \t Wins(player 1/ player 2):20.41%/73.47% \t Ties:6.12%%\n",
      "\n",
      "THE WINNER IS TorchPlayer(Vs_Multiple_v2- MCTS)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arena.play(n_games=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b6606-9813-47c0-90ee-eb1b7387bada",
   "metadata": {},
   "source": [
    "#### Se puede ver que player 2 (con Monte Carlo) fue bastánte superior a sin usarlo (casi 75% vs 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961789c7-1da9-4303-9d82-e0c7746d6c15",
   "metadata": {},
   "source": [
    "### Aclaración: En la notebook 010_Torneo hay varios match entre distintos modelos entrenados de diferentes maneras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
