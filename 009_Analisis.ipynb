{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3008c200-8288-4cd8-8bc0-f645d8314973",
   "metadata": {},
   "source": [
    "# Experimentos y Análisis de resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294b26d-e746-46df-abe3-51634722ce85",
   "metadata": {},
   "source": [
    "## Paso 1: Modelos Nuevos vs Random Player. Dos casos: Antes y después de buscar hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0180a-72b7-4175-acef-5d4f91d92b70",
   "metadata": {},
   "source": [
    "Se comparan 2 modelos que fueron entrenados 1 millon de timesteps:\n",
    "* Modelo rojo: Valores mayormente por defecto y numero de steps = 256 por rollout. Se usan 8 environments, es decir: 256 * 8 = 2048 steps por epoch, usando 10 epochs por buffer de experiencias.\n",
    "* Modelo celeste: Después de mejorar hiper parámetros se obtiene mayor reward, un clip fraction y divergencia KL menores, y la explained variance tiene mas sentido:\n",
    "    * Incremento de num steps a 2048 * 8 envs = 16k steps por epoch\n",
    "    * Incremento de 10 a 20 epochs por iteración\n",
    "    * Incremento de batch size de 64 a 128\n",
    "    * Disminución de learning rate de 0.0003 a 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43073595-bc64-4503-a2d0-cf7a8367f341",
   "metadata": {},
   "source": [
    "#### Rewards:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74be9e-c9fa-4004-a3c1-bffea389d297",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Random_Reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991438-8565-4bba-9993-d3d06add05e0",
   "metadata": {},
   "source": [
    "Curva celeste siempre por encima de la roja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f0234-9198-4e71-8f00-d6c2c21fe2b8",
   "metadata": {},
   "source": [
    "#### Losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42c974-a9ee-4bf9-8ece-a6faa3e9cd06",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Random_Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010826be-8020-45c0-acdb-57dc728362d3",
   "metadata": {},
   "source": [
    "Policy gradient loss: El celeste es mas cercano a 0. Interpreto que los ratios de policy / old_policy son mucho menores y por eso se manejan magnitudes mas bajas, y lo tomo como una mejora respecto al rojo (ver clip fraction y divergencia KL abajo).\n",
    "\n",
    "Value Loss: El rojo es menor, pero hay que tener en cuenta que son menores los rewards acumulados también."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5f0b1-81b0-4425-ac8d-6c931e2b2d5f",
   "metadata": {},
   "source": [
    "#### Clip Fraction y KL Divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fccc3-0aad-4d49-bf58-8b98a78a61a3",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Random_Clip_KL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14afc4f-8451-41cd-bbd1-c404dcef0182",
   "metadata": {},
   "source": [
    "Clip fraction y divergencia KL bastante inferiores en la celeste. Lo asocio a pasos mas pequeños donde la nueva policy es más parecida a la anterior que en la roja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09667-572e-4f48-98a2-c779544369d9",
   "metadata": {},
   "source": [
    "#### Explained Variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc493803-e2fb-4c99-bf5d-ed7d3ca39db0",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Random_Var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dad093-e934-4306-a326-f73a60442076",
   "metadata": {},
   "source": [
    "La explained variance de la roja no tiene sentido (oscila en valores positivos y negativos). La celeste se mantiene siempre encima de 0 y más estable, con lo cual la estimación del value debería ser mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da38e4-cdeb-4ca0-aea6-07fa961dbebc",
   "metadata": {},
   "source": [
    "## Paso 2: Tercera Variante contra Random. Achicar la red, entrenar mas, mejorar aprox de Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18f543-f791-4a5d-a894-8c479abfd8f2",
   "metadata": {},
   "source": [
    "* Redes mas chicas para policy y value (32 x 32 en vez de default 64 x 64). Siempre MLP\n",
    "* Clipping sobre value.\n",
    "* 2 millones de timesteps\n",
    "* Buscaba estimar mejor la value para subir el explained variance.\n",
    "* Nueva curva: Rosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3ec40-3eb4-4632-bd41-2924eb795977",
   "metadata": {},
   "source": [
    "#### Rewards:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4561e-e0b9-482a-a141-d5236b944024",
   "metadata": {},
   "source": [
    "![title](images/Analysis/3rd_model_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53338806-e868-4ee1-8000-05f3247c476c",
   "metadata": {},
   "source": [
    "El reward parece mas inestable que en los casos anteriores, sube mas lento que el celeste, pero termina ubicandose por encima y llega a un mejor modelo en los picos (toca 100% de victorias). También fue entrenado mas tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7728a-b16d-4acf-953c-6828a2c4b0a5",
   "metadata": {},
   "source": [
    "#### Losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a130200-f731-43f4-995e-0d1da83d6bcc",
   "metadata": {},
   "source": [
    "![title](images/Analysis/3rd_model_losses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea3c3c-87e2-432a-b010-9db041329c0a",
   "metadata": {},
   "source": [
    "La policy loss de la rosa se acerca a 0 a medida que pasan los timesteps. El ratio debe ser cada vez mas chico.\n",
    "\n",
    "La value loss de la rosa es bastante inferior y mas estable que en los casos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fff3e-60ff-488d-bf09-e1add3dc7632",
   "metadata": {},
   "source": [
    "#### Explained Variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689d8b1-830b-4f02-a7f7-4df6f2cd76b4",
   "metadata": {},
   "source": [
    "![title](images/Analysis/3rd_model_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c1647-e265-46ff-8fe3-5c2aa1101dfb",
   "metadata": {},
   "source": [
    "La explained variance parece estar por encima que en los casos anteriores y ser siempre positiva despues del \"warm-up\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e611632-e79b-444c-acb3-3c7fce0eda90",
   "metadata": {},
   "source": [
    "## Paso 3: Modelo nuevo (de cero) pero contra Random, Greedy y los 3 anteriores a la vez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31763920-b9d1-46f3-b37f-b862f8ec0a81",
   "metadata": {},
   "source": [
    "La forma de implementarlo fue mediante la opción local_player=\"multiple\" que agregué en CustomReversiModel (reversi_model.py), que permite pasar una carpeta en \"path_local_player\" en vez de un archivo. Todos los modelos que estén en esa carpeta + Greedy y Random son creados y repartidos entre los diferentes environments paralelos (cada env tiene un adversario diferente). En el buffer de experiencias quedan entonces mezcladas jugadas contra diferentes algoritmos.\n",
    "\n",
    "De esta manera el modelo aprende a jugar contra muchas estrategias diferentes al mismo tiempo.\n",
    "\n",
    "En el script \"learn.py\" se puede ver el uso de esta opción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52effe17-9762-4ced-afc0-c8d1a02c82f8",
   "metadata": {},
   "source": [
    "Aclaraciones:\n",
    "* Nueva curva: Verde\n",
    "* Los modelos adversarios estan en modo NO deterministico, para que haya mayor entropía y mas exploración. Si en evaluación tanto el modelo como adversario son deterministicos no entrena (gana o pierde siempre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798dd63-16f6-4498-b34b-b1dbd6f46846",
   "metadata": {},
   "source": [
    "#### Rewards:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c6c31-4d42-483d-b647-52129ddd2f93",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Multiple_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544462a-7e0c-4a91-a9e4-663c98f2aa3b",
   "metadata": {},
   "source": [
    "La reward del verde es primero negativa y mucho mas inestable que las otras. Considerar que es un modelo nuevo que juega contra diferentes estrategias que son mucho mejores que random y mas diversas. Luego se estabiliza en valores similares, pero esta vez jugando contra modelos mejores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bbf99-1d84-4d3f-8345-91a199621c59",
   "metadata": {},
   "source": [
    "#### Clip fraction y Divergencia KL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86dd07-1b3e-458a-b0e2-ffc09b1c6ba9",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Multiple_Clip_KL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a964dc6-1269-4f8b-9144-494a1117fe02",
   "metadata": {},
   "source": [
    "El clip version y la divergencia KL arrancan superiores al modelo celeste pero terminan siendo los mas bajos (lo asocio a un entrenamiento mas estable y el doble de timesteps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c97e7-28c5-4bfa-9437-5ebca6339d77",
   "metadata": {},
   "source": [
    "#### Losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef934f09-4f41-4c86-9f0a-d008bcbb4727",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Multiple_Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf558c98-9a58-4826-a0bf-9aab6f1b4c81",
   "metadata": {},
   "source": [
    "Para la loss de policy, el verde llega aun mas cerca de 0 que los anteriores (menor ratio entre new y old policy?)\n",
    "\n",
    "En el caso de la loss de value, pareciera que el nuevo modelo sabe estimar bastante mejor ya que se llega a una menor loss.\n",
    "Algo que noto es que la loss de la value primero baja, luego sube y finalmente baja en forma permanente y mas estable, a diferencia de los anteriores que solo bajaban. Atribuyo esta subida a la inestabilidad inicial en el mean reward (mayor diferencia entre lo estimado y lo real), hasta que se estabiliza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f45d15-6580-45d0-8fee-ae6324070d54",
   "metadata": {},
   "source": [
    "#### Explained Variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbf159-cb19-4f84-9c44-0884a0fdadf4",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Vs_Multiple_Var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf7b2e-11b2-4937-958a-88de915975ea",
   "metadata": {},
   "source": [
    "El explained variance de la verde es siempre superior a las otras curvas, lo cual debería indicar que el nuevo modelo estima mejor el valor de cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a10ad-c99e-4253-9e5b-1d80b38d0092",
   "metadata": {},
   "source": [
    "## Paso 4: Entrenamiento iterativo de modelo definitivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dde02e-cef3-4227-86b0-bf8fd0cf500d",
   "metadata": {},
   "source": [
    "El procedimiento realizado es:\n",
    "* 1 - Entrenar varios modelos nuevos contra random (pasos 1 y 2 descritos arriba).\n",
    "* 2 - Entrenar un modelo nuevo contra los anteriores con la opción \"multiple\" descrita arriba (paso 3).\n",
    "* 3 - Jugar un torneo (ver tournament.py, play_tournament.py y la notebook 010_Torneo) y quedarme con el mejor.\n",
    "* 4 - Cargar el mejor modelo y entrenarlo contra si mismo y los siguientes mejores modelos según resultados del torneo (2do, 3ro, etc). El objetivo de entrenar contra varios modelos es meter un poco de entropía y evitar caer en un óptimo local.\n",
    "* 5 - Repetir pasos 3 y 4\n",
    "* 6 - El resultado es el modelo final entrenado para competir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860da46-f7f1-44ac-a545-906d67aa63b7",
   "metadata": {},
   "source": [
    "El siguiente es un ejemplo luego de varias iteraciones del procedimiento anterior. El nuevo modelo es el naranja:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98ea9a-182d-45b4-a044-a907c7cb142a",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Final_model_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a95edb-9381-4076-8d30-495541aae70e",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Final_model_clip_kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447b52f-9656-4af0-8adc-d25068d474f6",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Final_model_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9f978-45ba-4b4a-b2bb-ad20758d4b8f",
   "metadata": {},
   "source": [
    "![title](images/Analysis/Final_model_losses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15596e6-2b65-417b-ad9e-0b8ee117a192",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "* Modelo que ya arranca entrenado (fui cargando modelos anteriores). Los primeros rewards estan por encima de 0.7 por ese motivo.\n",
    "* Reward: Mucho mas inestable e inferior => Pero considerar que juega contra muchos modelos buenos a la vez!\n",
    "* Clip Fraction y KL: Menores a los modelos anteriores. Menos cambios.\n",
    "* Explained variance: Bastante estable. Similar a la convergencia del verde.\n",
    "* Losses: Policy mas cercano a 0 => Menos ratio entre policies nueva y vieja. Value converge a un valor superior a las curvas rosas y verde.\n",
    "* A partir de 3M de iteraciones parece no mejorar en ninguna métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487b997-6acf-4d89-ba43-bd1ce8b3322d",
   "metadata": {},
   "source": [
    "## Ejercicio Adicional: CNN\n",
    "\n",
    "* Se realiza un entrenamiento (3 millones de timesteps) contra Random utilizando un features extractor con una red CNN, que servirá como features de entrada a un MLP de 32x32 (redes de policy y value de PPO), y es comparado con utilizar solo un flatten como features extractor y luego directo la red 32x32.\n",
    "* Curva azul es MLP 32x32 solo, curva gris es CNN + MLP.\n",
    "* Arquitectura de la red CNN:\n",
    "\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 16, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 8, kernel_size=4, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten()\n",
    "        \n",
    "\n",
    "* Ver: CNNFeaturesExtractor en custom_features_extractor.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8284c2-076f-4b6e-89e6-e53fb9123fea",
   "metadata": {},
   "source": [
    "![title](images/Analysis/CNN_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81c100-6a5e-4373-ba93-887ad7dfc50b",
   "metadata": {},
   "source": [
    "![title](images/Analysis/CNN_clip_KL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc23f7-09e9-4e23-890c-f2b57782de30",
   "metadata": {},
   "source": [
    "![title](images/Analysis/CNN_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcc05e-2c9d-4948-ab02-5478303dfc27",
   "metadata": {},
   "source": [
    "![title](images/Analysis/CNN_losses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d7bca-37f7-4b33-ac03-2a72779d53be",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "* Si bien las métricas de explained variance y value loss son mejores en CNN, el mean reward pareciera ser más ruidoso. El valor conseguido contra Random es bastante peor que en el caso de solo MLP y converge mas lento.\n",
    "* La versión con CNN tiene mas parámetros a entrenar que la MLP. Además de los 32x32 propios de las redes de value y policy (que yo elegí en ambos casos), se agregan 3 capas convolucionales + 1 lineal que adapta los canales de salida de la CNN a la dimensión esperada por PPO (observation_space). Considero que una red mas grande necesita más volumen de experiencias e iteraciones para converger. En un primer experimento de achicar la red de 64x64 a 32x32 funcionó mejor la segunda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a0f26-1a29-4ebb-85b9-c17e46a4926c",
   "metadata": {},
   "source": [
    "### Nota: Los modelos y los logs se pueden encontrar en las carpetas: selected_models y selected_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
